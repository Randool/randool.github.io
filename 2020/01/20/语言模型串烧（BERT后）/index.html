<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" integrity="sha256-yIDrPSXHZdOZhAqiBP7CKzIwMQmRCJ8UeB8Jo17YC4o=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"randool.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.19.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="语言模型的发展实在是太快了，怎么才赶上NLP快车呢？此文包含在BERT之后提出的一些语言模型。">
<meta property="og:type" content="article">
<meta property="og:title" content="语言模型串烧（BERT后）">
<meta property="og:url" content="https://randool.github.io/2020/01/20/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%B2%E7%83%A7%EF%BC%88BERT%E5%90%8E%EF%BC%89/index.html">
<meta property="og:site_name" content="More is different.">
<meta property="og:description" content="语言模型的发展实在是太快了，怎么才赶上NLP快车呢？此文包含在BERT之后提出的一些语言模型。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2020-01-20T03:50:43.000Z">
<meta property="article:modified_time" content="2020-02-09T12:57:36.000Z">
<meta property="article:author" content="Randool">
<meta property="article:tag" content="Notes">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://randool.github.io/2020/01/20/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%B2%E7%83%A7%EF%BC%88BERT%E5%90%8E%EF%BC%89/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://randool.github.io/2020/01/20/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%B2%E7%83%A7%EF%BC%88BERT%E5%90%8E%EF%BC%89/","path":"2020/01/20/语言模型串烧（BERT后）/","title":"语言模型串烧（BERT后）"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>语言模型串烧（BERT后） | More is different.</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">More is different.</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>







</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Transformer-XL%EF%BC%882019%E5%B9%B41%E6%9C%88%EF%BC%89"><span class="nav-number">1.</span> <span class="nav-text">Transformer-XL（2019年1月）</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#GPT-2%EF%BC%882019%E5%B9%B42%E6%9C%88%EF%BC%89"><span class="nav-number">2.</span> <span class="nav-text">GPT-2（2019年2月）</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ERNIE"><span class="nav-number">3.</span> <span class="nav-text">ERNIE</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#XLNet%EF%BC%882019%E5%B9%B46%E6%9C%88%EF%BC%89"><span class="nav-number">4.</span> <span class="nav-text">XLNet（2019年6月）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Permutation-Language-Model"><span class="nav-number">4.1.</span> <span class="nav-text">Permutation Language Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Two-stream-Self-Attention"><span class="nav-number">4.2.</span> <span class="nav-text">Two-stream Self-Attention</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#RoBERTa%EF%BC%882019%E5%B9%B47%E6%9C%88%EF%BC%89"><span class="nav-number">5.</span> <span class="nav-text">RoBERTa（2019年7月）</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CTRL%EF%BC%882019%E5%B9%B49%E6%9C%88%EF%BC%89"><span class="nav-number">6.</span> <span class="nav-text">CTRL（2019年9月）</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ALBERT%EF%BC%882019%E5%B9%B49%E6%9C%88%EF%BC%89"><span class="nav-number">7.</span> <span class="nav-text">ALBERT（2019年9月）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E8%A7%A3%E7%9F%A9%E9%98%B5"><span class="nav-number">7.1.</span> <span class="nav-text">分解矩阵</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E5%85%B1%E4%BA%AB"><span class="nav-number">7.2.</span> <span class="nav-text">参数共享</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%A5%E9%97%B4%E8%BF%9E%E8%B4%AF%E6%80%A7%E6%8D%9F%E5%A4%B1"><span class="nav-number">7.3.</span> <span class="nav-text">句间连贯性损失</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Reformer%EF%BC%882020%EF%BC%89"><span class="nav-number">8.</span> <span class="nav-text">Reformer（2020）</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#AdaBERT%EF%BC%882020%EF%BC%89"><span class="nav-number">9.</span> <span class="nav-text">AdaBERT（2020）</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-number">10.</span> <span class="nav-text">参考</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Transformer-XL"><span class="nav-number">10.1.</span> <span class="nav-text">Transformer-XL</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XLNet"><span class="nav-number">10.2.</span> <span class="nav-text">XLNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RoBERTa"><span class="nav-number">10.3.</span> <span class="nav-text">RoBERTa</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ALBERT"><span class="nav-number">10.4.</span> <span class="nav-text">ALBERT</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reformer"><span class="nav-number">10.5.</span> <span class="nav-text">Reformer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#AdaBERT"><span class="nav-number">10.6.</span> <span class="nav-text">AdaBERT</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B6%E4%BB%96"><span class="nav-number">10.7.</span> <span class="nav-text">其他</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Randool"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Randool</p>
  <div class="site-description" itemprop="description">More is different.</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">35</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">36</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/Randool" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Randool" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:dlf43@qq.com" title="E-Mail → mailto:dlf43@qq.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
    <div class="sidebar-inner sidebar-blogroll">
      <div class="links-of-blogroll animated">
        <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
          链接
        </div>
        <ul class="links-of-blogroll-list">
            <li class="links-of-blogroll-item">
              <a href="http://zrawberry.com/" title="http:&#x2F;&#x2F;zrawberry.com&#x2F;" rel="noopener" target="_blank">草莓君</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://www.aeonni.com/" title="https:&#x2F;&#x2F;www.aeonni.com&#x2F;" rel="noopener" target="_blank">Aeonni</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="http://me.csdn.net/Smile_coderrr/" title="http:&#x2F;&#x2F;me.csdn.net&#x2F;Smile_coderrr&#x2F;" rel="noopener" target="_blank">Smile_coderrr</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://nan01ab.github.io/archive?tdsourcetag=s_pctim_aiomsg" title="https:&#x2F;&#x2F;nan01ab.github.io&#x2F;archive?tdsourcetag&#x3D;s_pctim_aiomsg" rel="noopener" target="_blank">nan01ab头条大佬</a>
            </li>
        </ul>
      </div>
    </div>
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://randool.github.io/2020/01/20/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%B2%E7%83%A7%EF%BC%88BERT%E5%90%8E%EF%BC%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Randool">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="More is different.">
      <meta itemprop="description" content="More is different.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="语言模型串烧（BERT后） | More is different.">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          语言模型串烧（BERT后）
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-01-20 11:50:43" itemprop="dateCreated datePublished" datetime="2020-01-20T11:50:43+08:00">2020-01-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2020-02-09 20:57:36" itemprop="dateModified" datetime="2020-02-09T20:57:36+08:00">2020-02-09</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>语言模型的发展实在是太快了，怎么才赶上NLP快车呢？此文包含在BERT之后提出的一些语言模型。</p>
<span id="more"></span>

<h1 id="Transformer-XL（2019年1月）"><a href="#Transformer-XL（2019年1月）" class="headerlink" title="Transformer-XL（2019年1月）"></a>Transformer-XL（2019年1月）</h1><p>为了方便陈述，用Trm-XL代表Transformer-XL。</p>
<p>Trm模型最大的缺点就是只能处理非常短的文本。而作为Trm的升级版，Trm-XL（XL是指extra long）在长距离依赖上表现十分好，并且在速度方面比Trm快1800多倍。</p>
<p>早在Trm-XL之前，就已经有一款称作Vanilla Trm的模型，将长文本分成一段段的chunk，然后再独立运行Trm，试图弥补Trm的近视。但是这样的措施实际上没有并没有太大的性能提升，首先在chunk与chunk之间，信息并不能交互，一旦有两个token之间的距离超过了Trm的边界，那么就是不相关的。其次在评估阶段，每过一个step就要运行一次Trm，时间消耗是很大的。</p>


<p>Trm-XL在Vanilla Trm的基础上引入了两个创新点：</p>
<ul>
<li>循环机制</li>
<li>相对位置编码</li>
</ul>
<p>以修复Vanilla Trm的短板。</p>
<p>Trm-XL虽然和Vanilla Trm一样也会对文本分段建模，但是引入了段与段之间的循环机制，上一个chunk的隐含层信息被缓存起来，使得当前chunk在计算时能利用之前的信息。这样一来，Trm的边界被拓宽了，就以下面这张图为例，x12的信息能追溯至x3。另外，有了之前chunk的部分信息，下一个chunk就不需要重复计算这些联系，所以每次向前都是迭代一个chunk的大小，不再是龟速的每次一个step了。</p>


<p>Trm-XL的另一个改进是使用了相对位置编码的position embedding。对于同一个chunk中的询问q_i 和键k_j，他们之间的注意力分数可以分解为：</p>


<p>其中$E_{x_i}$和$E_{x_j}$分别是词i和词j的embedding，$U_i$和$U_j$是位置向量，这个式子实际上是</p>
<p>$$\left(W_q (E_{x_i} + U_i )\right)^T \left(W_k (E_{x_j} + U_j)\right)$$</p>
<p>的展开式。而引入了相对位置编码后，原先所有的U将被替换，不过可以看到这种替换不是对称的，$A_{i,j}^rel$中所有的$U_j$被替换为$R_{i−j}$，而$U_i$则被替换为了u和v。另外，权重矩阵也有些许变化。</p>


<p>为什么这样做呢？实际上作者的意思是将位置i看作相对位置坐标系的原点，一旦i固定，j和i的关系就可以通过距离差确定了，上式中的R就是一个正弦信号嵌入矩阵。另外只要距离相同，不论i和j的实际数值是多少，都可以分享参数u和v，这应该是学习坐标原点的含义吧。</p>
<h1 id="GPT-2（2019年2月）"><a href="#GPT-2（2019年2月）" class="headerlink" title="GPT-2（2019年2月）"></a>GPT-2（2019年2月）</h1><p>OpenAI的GPT-2在写作上可谓是和人类难分真假，甚至有人想用GPT-2续写小说。那么这么强大的网络是怎么构造的呢？</p>
<p>一般来说，双向语言模型能比单向的提取更多的上下文信息，但是双向语言模型不太适合做生成任务，因在看不到后文的情况下，双向语言模型的性能会大打折扣。但是只用单向语言模型的GPT-2怎么吸引大众的眼球呢？在模型“先天不足”的情况下，作者只好选择用数据量来弥补。作者为了训练该网络，爬取了800w网页得到一份巨大的WebText数据集，然后堆叠了48层Trm（完全形态），形成一个拥有15亿参数的GPT-2。之前提到的Trm是会对一个chunk中所有的token对做self-attention的，但是为了保证单向的特性，作者只让token与之前已经出现的token做attention，也就是下图展示的结构了。</p>


<p>光有数据和模型还不够，还缺训练方法。在“_Improving Language Understanding by Generative Pre-Training_”中，也就是GPT1.0的训练分为无监督预训练（pre-training）和有监督微调（fine-tuning）两个阶段。无监督阶段就是很常规的给定上文预测下文的任务：</p>
<p>$$<br>L_1 (U) &#x3D; \sum_i{\log⁡ P(u_i |u_{i−k}, \dots, u_{i−1},\Theta)}<br>$$</p>
<p>其中U表示语料库。</p>
<p>而有监督阶段实际上是为了训练模型，使其适合其他任务。假设有一个已经标注的语料库C，假设每个文本由多个token序列$x_1,…,x_m$组成，target是y，该阶段的目标函数可以抽象地用下面的式子表示</p>
<p>$$<br>L_2(C) &#x3D; \sum_{x,y}{log⁡P(y|x_1,…,x_m )}<br> &#x3D; \sum_{x,y}{\log⁡ softmax(h_l^m W_y)}<br>$$</p>
<p>上式W是加在最后一层Trm后的全连接层，是该阶段的训练参数。</p>
<p>一图胜千言，论文中给出了一张图，左边表示模型的结构，右边表示各种任务。</p>


<p>可以看到模型结构部分的输出有两部分，分别对应上面说到的无监督和有监督训练。而为了更好的训练效果，在有监督训练阶段还会使用无监督的目标函数，让模型“不忘初心”。</p>
<p>$$ L_3 (C) &#x3D; \lambda \times L_1(C) + L_2(C) $$</p>
<p>GPT-2结构貌似和GPT相比没有太大的区别，就增加了Layer Normalization和权重初始化方式。</p>
<h1 id="ERNIE"><a href="#ERNIE" class="headerlink" title="ERNIE"></a>ERNIE</h1><h1 id="XLNet（2019年6月）"><a href="#XLNet（2019年6月）" class="headerlink" title="XLNet（2019年6月）"></a>XLNet（2019年6月）</h1><p>在提出XLNet的论文中，作者分析了BERT成功的原因：自编码模型（AE）能更好地利用上下文信息。但同时，作者也指出了BERT的缺陷：pre-training阶段引入的[mask]在fine-tuning阶段没有用到，这实际上会影响BERT的性能。基于这一点，作者提出了XLNet，其核心思想可以概括为：__一个用类似AE的方式做预训练的AR模型__。</p>
<h2 id="Permutation-Language-Model"><a href="#Permutation-Language-Model" class="headerlink" title="Permutation Language Model"></a>Permutation Language Model</h2><p>为了让AR模型同时看到上下文，作者提出了permutation LM。假设有一个句子序列<code>(a,b,c,d,e)</code>，并且d是待预测词，那么做法就是将d固定但将其他词打乱，假设现在得到了<code>(b,e,a,d,c)</code>，那么使用AR模型预测d这个词的时候就有一定几率看到下文了。这么做确实弥补了AR模型偏袒上文的缺点，但是却引入了另一个问题——句子序列退化成了词袋模型。在BERT中是用position embedding解决这个问题的，XLNet却用了另一种解决方案。作为对比，传统AR模型，比如Trm等，预测当前位置的概率采用的是简单直接的Softmax<br>$$<br>p_{\theta}(X_{z_t} &#x3D; x | x_{z_{&lt;t}}) &#x3D; \frac{\exp⁡(e(x)^T h_{\theta}(x_{z&lt;t}))}<br>{\sum_{x^′}{\exp⁡(e(x^′)^T h_{\theta}(x_{z&lt;t}))}}<br>$$</p>
<p>而XLNet采用的则是下面的概率：<br>$$<br>p_{\theta}(X_{z_t} &#x3D; x | x_{z_{&lt;t}}) &#x3D; \frac{\exp⁡(e(x)^T g_{\theta}(x_{z&lt;t},z_t))}<br>{\sum_{x^′}{\exp⁡(e(x^′)^T g_{\theta}(x_{z&lt;t},z_t))}}<br>$$</p>
<p>也就是提出了一种新的位置表示方式，这样即使经过重排列，原位置信息还是能被包含在内。这样，XLNet就能在同时看到上下文的时候不受重排列的影响了。顺便说一下，虽然直接对原句子排列省时省力，但是XLNet吃的数据中，句子序列还是原来的顺序，只不过在用Trm提取信息前用一层mask将数据处理了一下，使其在Trm眼中就像经过了排列那样。以排列(3,2,4,1)为例，这个mask可以长这样，有颜色的表示“可以看到”，空白表示“看不到”。</p>



<p>然后将模型展开后，不同位置能看到的信息就像下面这张图一样</p>


<h2 id="Two-stream-Self-Attention"><a href="#Two-stream-Self-Attention" class="headerlink" title="Two-stream Self-Attention"></a>Two-stream Self-Attention</h2><p>但是这里还存在一个问题，以上图为例，当模型要预测x3这个位置的词时，x3自身不应该作为模型的上下文信息被输入，否则输入啥输出啥的模式训练出来的模型不work。如果不将x3输入，那么x3的第1层隐含层信息$h_3^{(1)}$中就不包括x3了，可是x3作为序列(3,2,4,1)中的第一个token，后面的token都等着吃x3的信息呢，如果$h_3^{(1)}$中不包含x3的信息，那么之后的位置就无法提取有效的上下文信息！</p>
<p>这样一方面不能让看到“自身”，又要让模型target-aware，矛盾双方牵扯出一种解决思路——two-stream，即其中一条路包含“自身”，另一条路不包含“自身”。其实上图中h那条路包含自身信息，这条stream被称为content-stream，这条路实际上就是最基础的attention；而g那条路不包含自身信息，被称为query-stream。</p>


<p>$$<br>g_{z_t}^{(m)} &#x3D; Attention\left( Q&#x3D;g_{z_t}^{(m-1)},KV&#x3D;h_{Z_{&lt;t}}^{(m-1)} \right)<br>$$</p>
<p>$$<br>h_{z_t}^{(m)} &#x3D; Attention\left( Q&#x3D;h_{z_t}^{(m-1)},KV&#x3D;h_{Z_{\le t}}^{(m-1)} \right)<br>$$</p>
<p>上面两个式子中，query stream中包含位置信息$z_t$但不包含$x_{z_t}$；而content stream同时包含位置信息$z_t$和$x_{z_t}$。</p>
<p>其实个人感觉permutation LM会因为token的顺序多变导致训练困难，而作者也在论文中提到了训练的技巧，那就是只预测句子后后半部分，这样做可以减小训练难度，也可以节约时间空间。</p>
<h1 id="RoBERTa（2019年7月）"><a href="#RoBERTa（2019年7月）" class="headerlink" title="RoBERTa（2019年7月）"></a>RoBERTa（2019年7月）</h1><h1 id="CTRL（2019年9月）"><a href="#CTRL（2019年9月）" class="headerlink" title="CTRL（2019年9月）"></a>CTRL（2019年9月）</h1><h1 id="ALBERT（2019年9月）"><a href="#ALBERT（2019年9月）" class="headerlink" title="ALBERT（2019年9月）"></a>ALBERT（2019年9月）</h1><p>ALBERT是谷歌发布的轻量级BERT模型，比BERT模型参数小18倍，性能还超越了它，在SQuAD和RACE测试上创造了新的SOTA。ALBERT主要是三个方面的工作：</p>
<h2 id="分解矩阵"><a href="#分解矩阵" class="headerlink" title="分解矩阵"></a>分解矩阵</h2><p>说得专业一些就是 __Factorized embedding parameterization__。在BERT中，词向量维度E和隐含层维度H是相等的，而作者认为预训练模型的重点是在训练模型提取上下文的能力上，而不需要花太多空间在“上下文无关”的词向量上，因此ALBERT中<code>E&lt;&lt;H</code>，但为了保证之后计算过程中维度的一致性，必须添加一个隐含层，将E维词向量映射回H维。更直白一些就是将原本V<em>H的矩阵分解为V</em>E和E*H两个矩阵，并且只要E足够小，这个空间就被省下来了。(⊙ˍ⊙)真是朴素……</p>
<p>那么这个操作能省下多少空间呢？假设词典大小是3万，BERT-large中E和H都是1024维，那么参数量大概是<code>30000*1024=30720000</code>。而ALBERT-large的E是128维，H保持不变，那么参数量为<code>30000*128+128*1024=3971072</code>。两者相差26748928，大概是27M，并没有达到18倍的参数量减小。就算加上position embedding，所节省的空间也大致如此，光靠分解矩阵节省的空间是很有限的，作者一定用了其他节省空间的方法。</p>


<h2 id="参数共享"><a href="#参数共享" class="headerlink" title="参数共享"></a>参数共享</h2><p>有一种针对Transformer的改进方式是使用单独一个Trm单元，然后让其在时间尺度上展开，使其具有和RNN一样的结构，这种改进后的模型称为Universal Transformer。ALBERT也用了类似的思想，但是并不是在时间尺度上，而是在模型深度上共享单元。具体分为三种模式：all-shared、shared-attention、shared-FFN。</p>


<p>如果分享模式是all-shared，那么模型相当于由12个完全相同的层堆叠而成，显而易见这样做能节省大量的空间。但问题是这样的模型能work吗？上面给出的实验结果已经给出了答案，all-shared模型相比于原先的模型在各种测试下会有轻微的性能下滑，但是相比于节省的空间，这种下滑还是值得的。另外注意到shared-attention模型不仅减小了参数量，甚至在不少任务下还有性能的提升。</p>
<h2 id="句间连贯性损失"><a href="#句间连贯性损失" class="headerlink" title="句间连贯性损失"></a>句间连贯性损失</h2><p>BERT预训练使用NSP（next-sentence prediction）损失函数，其用于判断一个句子对中的两个句子是否是语义连续的。具体来说，NSP是一个二分类任务，其中每个正例是由两句连续的句子组成，而每个负例由来自不同文章的两个句子构成。BERT使用这样的损失函数是希望帮助可以其在自然语言推理任务上获得更好的性能。但是ALBERT的作者认为NSP太过简单，因为预测主题比预测连贯性更容易，所以模型更可能是通过判断两个句子的主题间接地预测了连贯性。ALBERT采用的SOP（sentence-order prediction）就是再NSP基础上对负例的构造做了小改进，既然模型会偷懒选择预测主题，那么只要是来自相同主题的两个句子，按照错误的顺序排列就可以构成一个负例。又是朴素的思想ㄟ( ▔, ▔ )ㄏ</p>
<p>作者最后给出了BERT和ALBERT在各种任务上的表现</p>


<p>可以看到ALBERT-base因为参数量小，因此和BERT最好的模型——BERT-large有比较大的性能差距，但是ALBERT-large的表现和BERT-large相似，考虑到ALBERT-large将近18倍的“瘦身”以及6.5倍的加速，可以认为ALBERT模型还是很成功的。或许是选用了更合适下游任务的损失函数，ALBERT在阅读理解任务RACE上的表现有较大的提升，或者反过来说明BERT的损失函数选的不好？此外，比较BERT和ALBERT在参数量增加后性能提升的趋势可以看出，ALBERT至少还是存在提升的潜力的。</p>
<h1 id="Reformer（2020）"><a href="#Reformer（2020）" class="headerlink" title="Reformer（2020）"></a>Reformer（2020）</h1><h1 id="AdaBERT（2020）"><a href="#AdaBERT（2020）" class="headerlink" title="AdaBERT（2020）"></a>AdaBERT（2020）</h1><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><h2 id="Transformer-XL"><a href="#Transformer-XL" class="headerlink" title="Transformer-XL"></a>Transformer-XL</h2><ul>
<li>Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q., &amp; Salakhutdinov, R. (2019). Transformer-XL: Attentive Language Models beyond a Fixed-Length Context. 2978–2988. <a target="_blank" rel="noopener" href="https://doi.org/10.18653/v1/p19-1285">https://doi.org/10.18653/v1/p19-1285</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/Magical_Bubble/article/details/89060213">Transformer-XL解读（论文 + PyTorch源码）</a></li>
</ul>
<h2 id="XLNet"><a href="#XLNet" class="headerlink" title="XLNet"></a>XLNet</h2><ul>
<li>Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R., &amp; Le, Q. V. (2019). XLNet: Generalized Autoregressive Pretraining for Language Understanding. 1–18. Retrieved from <a target="_blank" rel="noopener" href="http://arxiv.org/abs/1906.08237">http://arxiv.org/abs/1906.08237</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_37947156/article/details/93035607">XLNet原理解读</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/70257427">XLNet:运行机制及和Bert的异同比较</a></li>
</ul>
<h2 id="RoBERTa"><a href="#RoBERTa" class="headerlink" title="RoBERTa"></a>RoBERTa</h2><ul>
<li>Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., … Stoyanov, V. (2019). RoBERTa: A Robustly Optimized BERT Pretraining Approach. (1). Retrieved from <a target="_blank" rel="noopener" href="http://arxiv.org/abs/1907.11692">http://arxiv.org/abs/1907.11692</a></li>
</ul>
<h2 id="ALBERT"><a href="#ALBERT" class="headerlink" title="ALBERT"></a>ALBERT</h2><ul>
<li>Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., &amp; Soricut, R. (2019). ALBERT: A Lite BERT for Self-supervised Learning of Language Representations. 1–17. Retrieved from <a target="_blank" rel="noopener" href="http://arxiv.org/abs/1909.11942">http://arxiv.org/abs/1909.11942</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/chivalrousli/article/details/103307216">ALBERT 思想简介</a></li>
</ul>
<h2 id="Reformer"><a href="#Reformer" class="headerlink" title="Reformer"></a>Reformer</h2><h2 id="AdaBERT"><a href="#AdaBERT" class="headerlink" title="AdaBERT"></a>AdaBERT</h2><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ul>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247535966&idx=4&sn=7ada47836c1a41fbb72593f8b6891d44&chksm=e8d0fa2cdfa7733a45c3e1a552a99da537dcda6629c8f63fbc9dd1b381ad1898a230b1db2e7d&scene=90&xtrack=1&subscene=93&clicktime=1578375620&enterid=1578375620&ascene=56&devicetype=android-29&version=27000a34&nettype=WIFI&abtest_cookie=AAACAA==&lang=zh_CN&exportkey=AebBGfR5OYkTM082tSC+3gw=&pass_ticket=ideaSFADceEvZmAzNk5QFyZ9Pd+WZmrBQP6XhG9m6yCIiqfLvKgnj9pUfjPkVdjW&wx_header=1">一文看尽2019年NLP前沿突破</a></li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Notes/" rel="tag"># Notes</a>
              <a href="/tags/AI/" rel="tag"># AI</a>
              <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
              <a href="/tags/NLP/" rel="tag"># NLP</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2020/01/14/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%B2%E7%83%A7%EF%BC%88BERT%E5%89%8D%EF%BC%89/" rel="prev" title="语言模型串烧（BERT前）">
                  <i class="fa fa-angle-left"></i> 语言模型串烧（BERT前）
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2020/02/05/Prefix%20Beam%20Search/" rel="next" title="Prefix Beam Search">
                  Prefix Beam Search <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Randool</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  






  





</body>
</html>
